{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3373f2c7-da8e-49eb-8300-25ae4a00fdaa",
   "metadata": {},
   "source": [
    "# Welcome to lumigator foxfooding!\n",
    "\n",
    "## Agenda\n",
    "\n",
    "+ Introduction and setup environment (credentials)\n",
    "+ Platform Setup and Walkthrough\n",
    "+ Explanation of and Examination of Thunderbird Ground Truth\n",
    "+ Model Selection ( 1 encoder/decoder), (2 decoder), eval against GPT4\n",
    "+ Run experiment and show results\n",
    "+ Evaluate results and discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f309f-5074-4fca-8fc7-974c4e35bee4",
   "metadata": {},
   "source": [
    "## Foxfooding Introduction and Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b47fb-996a-4931-921f-4027e5930c83",
   "metadata": {},
   "source": [
    "## Who we are, what we do, about the platform, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee9839-3102-47e4-8bb5-0cbd8c68f537",
   "metadata": {},
   "source": [
    "## Platform Setup and Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237ee62-33d1-480d-864f-03f70537c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lumigator_demo as ld\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wrap columns for inspection\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# stylesheet for visibility\n",
    "plt.style.use(\"fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c813-36f8-4439-880d-441499e80493",
   "metadata": {},
   "source": [
    "Write your team name below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67092b57-e4ff-4fd7-a46b-f6ab9c56c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"lumigator_enthusiasts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1493b-ecbd-4b2c-a13a-58365cc15023",
   "metadata": {},
   "source": [
    "## Generating Data for Ground Truth Evaluation\n",
    "\n",
    "In order to generate a ground truth summary for our data, we first need an input dataset. In this case we use threads from the [Thunderbird public mailing list.](https://thunderbird.topicbox.com/latest).  In order to generate the ground truth and then later evaluate the model, we need at least 100 samples to start with, where a sample is a single email or single email conversation.\n",
    "\n",
    "Our selection criteria: \n",
    "\n",
    "+ Collect 100 samples of email thread conversations, as recent as possible and fairly complete so they can be evaluated\n",
    "+ Clean them of email formatting such as `>`\n",
    "+ One consideration here will be that BART, the baseline model we're using, accepts 1024 token context window as input, i.e.  we have to have input email threads that are ~ approximately 1000 words, so keeping on the conservative side\n",
    "\n",
    "Once we've collected them, we'd like to take a look at the data before we generate summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323a9e-aec4-402f-81c0-164f4f890723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T17:59:45.160392Z",
     "start_time": "2024-07-17T17:59:45.157516Z"
    }
   },
   "outputs": [],
   "source": [
    "# show information about the Thunderbird dataset\n",
    "dataset_id = \"db7ff8c2-a255-4d75-915d-77ba73affc53\"\n",
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab1a6c-30bc-421d-908d-20268cdefb98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:10.371832Z",
     "start_time": "2024-07-17T18:00:10.354800Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the dataset into a pandas dataframe\n",
    "df = ld.dataset_download(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ddccc-33ab-4204-967a-d266b8ff051e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:25.102809Z",
     "start_time": "2024-07-17T18:00:25.098826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "# we define the data with examples\n",
    "df['examples'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a4fa9-ce82-43ec-a2b1-0fc984b23cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:28.965534Z",
     "start_time": "2024-07-17T18:00:28.962726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a function to do some simple character counts for model input\n",
    "df['char_count'] = df['examples'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ca4cc-2586-4b36-99e4-ed8d879e5468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:30.382662Z",
     "start_time": "2024-07-17T18:00:30.377035Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9694e3-d4d3-46b6-9023-4744f1ab73e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:31.627488Z",
     "start_time": "2024-07-17T18:00:31.620802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28c016-5aae-47f2-a18e-e470cae8ef4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:33.594470Z",
     "start_time": "2024-07-17T18:00:33.477793Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e914253-e217-42f8-85a2-59f32daf6779",
   "metadata": {},
   "source": [
    "## Working with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66191-c874-4303-bd26-ef93388a124f",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "The following dataset is already in the format that we need as input: \n",
    "- one field called `examples` containing the text to summarize\n",
    "- one field called `ground_truth` containing the summaries to the models' outputs against\n",
    "\n",
    "Note that you can load many different types of file formats in a similar way (see https://huggingface.co/docs/datasets/loading#local-and-remote-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260eae7-f978-46ba-9994-da5734a02319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"thunderbird.csv\"\n",
    "\n",
    "### commented until the dataset is final - just use the following cell to download the dataset\n",
    "# ds = load_dataset(\"csv\", data_files = dataset_name, split=\"train\")\n",
    "# ds = ds.to_pandas()\n",
    "\n",
    "# show / do things with the dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0682d-0dd6-49b1-a4dc-8581760b1a59",
   "metadata": {},
   "source": [
    "## Dataset Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbcb60-5940-41e5-873f-b346f8bad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = ld.dataset_upload(dataset_name)\n",
    "# dataset_id = ld.get_resource_id(r)\n",
    "\n",
    "dataset_id = \"f5d54efa-247d-4910-9393-f6003da9fb68\" # thunderbird pre-saved dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4008d0-cdc3-403e-a46e-88cca708d91e",
   "metadata": {},
   "source": [
    "### Check dataset info\n",
    "\n",
    "At any point, one can get dataset info by just providing its UUID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba59f93-d5ac-416e-8ff4-79db147627c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a005f740",
   "metadata": {},
   "source": [
    "## Generating Data for Ground Truth Evaluation\n",
    "\n",
    "In order to generate a ground truth summary for our data, we first need an input dataset. In this case we use threads from the [Thunderbird public mailing list.](https://thunderbird.topicbox.com/latest).  In order to generate the ground truth and then later evaluate the model, we need at least 100 samples to start with, where a sample is a single email or single email conversation.\n",
    "\n",
    "Our selection criteria: \n",
    "\n",
    "+ Collect 100 samples of email thread conversations, as recent as possible and fairly complete so they can be evaluated\n",
    "+ Clean them of email formatting such as `>`\n",
    "+ One consideration here will be that BART, the baseline model we're using, accepts 1024 token context window as input, i.e.  we have to have input email threads that are ~ approximately 1000 words, so keeping on the conservative side\n",
    "\n",
    "Once we've collected them, we'd like to take a look at the data before we generate summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f72209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset into a pandas dataframe\n",
    "df = ld.dataset_download(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "df['examples'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9df712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some simple character counts for model input\n",
    "df['char_count'] = df['examples'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf901a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44586c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot character counts \n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614682c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with Mistral \n",
    "\n",
    "responses = []\n",
    "\n",
    "\n",
    "for sample in df['examples'][0:10]:\n",
    "    response = ld.get_mistral_ground_truth(sample)\n",
    "    print(sample, response.text)\n",
    "    responses.append((sample, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d140470",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_df = pd.DataFrame(responses, columns=['Original', 'Response'])\n",
    "\n",
    "mistral_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all deployments \n",
    "ld.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new deployment\n",
    "ld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0afa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with BART\n",
    "\n",
    "deployment_id = \"bd6e1a72-037e-4b76-ab5c-53adac282b1b\"\n",
    "\n",
    "for string in df['examples'][0:10]:\n",
    "    response = ld.get_bart_ground_truth(deployment_id,string)\n",
    "    print(string, response.text)\n",
    "    responses.append((string, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_results_df = pd.DataFrame(responses, columns=['Original', 'Response'])\n",
    "\n",
    "bart_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876b7f8-e645-4791-aa27-351e3296e2de",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "What you see below are different lists of models we have already tested for the summarization task.\n",
    "The `models` variable at the end provides you with a selection, but you can choose any combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcdf76-c9ad-4459-b0c3-49098286ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_models = [\n",
    "    'hf://facebook/bart-large-cnn',\n",
    "    'hf://mikeadimech/longformer-qmsum-meeting-summarization', \n",
    "    'hf://mrm8488/t5-base-finetuned-summarize-news',\n",
    "    'hf://Falconsai/text_summarization',\n",
    "]\n",
    "\n",
    "dec_models = [\n",
    "    'hf://mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    # TODO: test more dec_models such as\n",
    "    # 'hf://meta-llama/Meta-Llama-3-8B',\n",
    "    # 'hf://microsoft/Phi-3-mini-4k-instruct',\n",
    "]\n",
    "\n",
    "gpts = [\n",
    "    \"oai://gpt-4o-mini\",\n",
    "    \"oai://gpt-4-turbo\",\n",
    "    \"oai://gpt-3.5-turbo-0125\"  \n",
    "]\n",
    "\n",
    "models = [\n",
    "    # enc_dec_models[0],\n",
    "    dec_models[0],\n",
    "    # gpts[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af4e7-5a02-4e5a-b2f5-a065716ddbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74658955-4a41-4be1-b29d-8efa734bed9d",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73f6a1-7be9-43a7-8304-4cb6a408318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following to 0 to use all samples in the dataset\n",
    "max_samples = 94\n",
    "\n",
    "responses = []\n",
    "for model in models:\n",
    "    descr = f\"Testing {model} summarization model on {dataset_name}\"\n",
    "    responses.append(ld.experiments_submit(model, team_name, descr, dataset_id, max_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787190-4581-4c26-beca-0260303a68bd",
   "metadata": {},
   "source": [
    "### Track evaluation jobs\n",
    "\n",
    "Run the following to track your evaluation jobs.\n",
    "\n",
    "- *NOTE*: you won't be able to run other cells while this one is running. However, you can interrupt it whenever you want by clicking on the \"stop\" button above and run it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858bcea-6fcb-4441-b56e-5d89502f10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [ld.get_resource_id(r) for r in responses]\n",
    "\n",
    "wip = ld.show_experiment_statuses(job_ids)\n",
    "while wip == True:\n",
    "    time.sleep(5)\n",
    "    clear_output()\n",
    "    wip=ld.show_experiment_statuses(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8cbf3-e44f-4f1e-b520-3f6fac147fca",
   "metadata": {},
   "source": [
    "## Show evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709b8-59ba-4a9c-81bd-59b2fc612f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the jobs complete, gather evaluation results\n",
    "eval_results = []\n",
    "for job_id in job_ids:\n",
    "    eval_results.append(ld.experiments_result_download(job_id))\n",
    "\n",
    "# convert results into a pandas dataframe\n",
    "eval_table = ld.eval_results_to_table(models, eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf5e90-7fa2-4a7d-98bf-2ff5d736c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbdaec-a1e9-4731-bfdd-b471e6a0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9aff3-5d33-4cd0-a7f4-8cb0811155b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
