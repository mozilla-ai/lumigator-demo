{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3373f2c7-da8e-49eb-8300-25ae4a00fdaa",
   "metadata": {},
   "source": [
    "# Welcome to Lumigator foxfooding!\n",
    "\n",
    "## Agenda\n",
    "\n",
    "+ Setup\n",
    "+ Dataset Analysis \n",
    "+ Explanation of and Examination of Thunderbird Ground Truth\n",
    "+ Model Selection ( 1 encoder/decoder), (2 decoder), eval against GPT4\n",
    "+ Run experiment and show results\n",
    "+ Evaluate results and discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41dab1",
   "metadata": {},
   "source": [
    "## Platform Setup and Jupyter Walkthrough\n",
    "\n",
    "You'll be working in the Jupyter notebook and the platform itself is accessible via URL in the slide deck. To work with Jupyter, click \"run cell\" to run the code and see results below the cell you're currently running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8237ee62-33d1-480d-864f-03f70537c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lumigator_demo as ld\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wrap columns for inspection\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# stylesheet for visibility\n",
    "plt.style.use(\"fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c813-36f8-4439-880d-441499e80493",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "We're grouping experiments by team name to organize the data, pick a team name below and run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67092b57-e4ff-4fd7-a46b-f6ab9c56c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestion: \"lumigator_enthusiasts\", \"your team name etc\"\n",
    "team_name = TEAM_NAME_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1493b-ecbd-4b2c-a13a-58365cc15023",
   "metadata": {},
   "source": [
    "## Generating Data for Ground Truth Evaluation\n",
    "\n",
    "In order to generate a ground truth summary for our data, we first need an input dataset. In this case we use threads from the [Thunderbird public mailing list.](https://thunderbird.topicbox.com/latest).  In order to generate the ground truth and then later evaluate the model, we need at least 100 samples to start with, where a sample is a single email or single email conversation.\n",
    "\n",
    "Our selection criteria: \n",
    "\n",
    "+ Collect 100 samples of email thread conversations, as recent as possible and fairly complete so they can be evaluated\n",
    "+ Clean them of email formatting such as `>`\n",
    "+ One consideration here will be that BART, the baseline model we're using, accepts 1024 token context window as input, i.e.  we have to have input email threads that are ~ approximately 1000 words, so keeping on the conservative side\n",
    "\n",
    "Once we've collected them, we'd like to take a look at the data before we generate summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323a9e-aec4-402f-81c0-164f4f890723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T17:59:45.160392Z",
     "start_time": "2024-07-17T17:59:45.157516Z"
    }
   },
   "outputs": [],
   "source": [
    "# show information about the Thunderbird dataset\n",
    "dataset_id = \"db7ff8c2-a255-4d75-915d-77ba73affc53\"\n",
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab1a6c-30bc-421d-908d-20268cdefb98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:10.371832Z",
     "start_time": "2024-07-17T18:00:10.354800Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the dataset into a pandas dataframe\n",
    "df = ld.dataset_download(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ddccc-33ab-4204-967a-d266b8ff051e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:25.102809Z",
     "start_time": "2024-07-17T18:00:25.098826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "# we define the data with examples\n",
    "df['examples'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a4fa9-ce82-43ec-a2b1-0fc984b23cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:28.965534Z",
     "start_time": "2024-07-17T18:00:28.962726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a function to do some simple character counts for model input\n",
    "df['char_count'] = df['examples'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ca4cc-2586-4b36-99e4-ed8d879e5468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:30.382662Z",
     "start_time": "2024-07-17T18:00:30.377035Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9694e3-d4d3-46b6-9023-4744f1ab73e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:31.627488Z",
     "start_time": "2024-07-17T18:00:31.620802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28c016-5aae-47f2-a18e-e470cae8ef4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:33.594470Z",
     "start_time": "2024-07-17T18:00:33.477793Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe26e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with Mistral \n",
    "\n",
    "responses = []\n",
    "\n",
    "\n",
    "for sample in df['examples'][0:10]:\n",
    "    response = ld.get_mistral_ground_truth(sample)\n",
    "    print(sample, response.text)\n",
    "    responses.append((sample, response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_df = pd.DataFrame(responses, columns=['Original', 'Response'])\n",
    "\n",
    "mistral_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all deployments \n",
    "ld.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with BART\n",
    "\n",
    "# PUT DEPLOYMENT ID HERE \n",
    "deployment_id = \"2240dc80-5759-4e04-8aad-2d2eab2392e2\"\n",
    "\n",
    "for string in df['examples'][0:10]:\n",
    "    response = ld.get_bart_ground_truth(deployment_id,string)\n",
    "    print(string, response)\n",
    "    responses.append((string, response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e914253-e217-42f8-85a2-59f32daf6779",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66191-c874-4303-bd26-ef93388a124f",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "The following dataset is already in the format that we need as input: \n",
    "- one field called `examples` containing the text to summarize\n",
    "- one field called `ground_truth` containing the summaries to the models' outputs against\n",
    "\n",
    "Note that you can load many different types of file formats in a similar way (see https://huggingface.co/docs/datasets/loading#local-and-remote-files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0682d-0dd6-49b1-a4dc-8581760b1a59",
   "metadata": {},
   "source": [
    "## Dataset Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbcb60-5940-41e5-873f-b346f8bad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"thunderbird.csv\"\n",
    "dataset_id = \"f5d54efa-247d-4910-9393-f6003da9fb68\" # thunderbird pre-saved dataset HuggingFace\n",
    "\n",
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876b7f8-e645-4791-aa27-351e3296e2de",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "What you see below are different lists of models we have already tested for the summarization task.\n",
    "The `models` variable at the end provides you with a selection, but you can choose any combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcdf76-c9ad-4459-b0c3-49098286ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_models = [\n",
    "    'hf://facebook/bart-large-cnn',\n",
    "    'hf://mikeadimech/longformer-qmsum-meeting-summarization', \n",
    "    'hf://mrm8488/t5-base-finetuned-summarize-news',\n",
    "    'hf://Falconsai/text_summarization',\n",
    "]\n",
    "\n",
    "dec_models = [\n",
    "    'hf://mistralai/Mistral-7B-Instruct-v0.3',\n",
    "]\n",
    "\n",
    "gpts = [\n",
    "    \"oai://gpt-4o-mini\",\n",
    "    \"oai://gpt-4-turbo\",\n",
    "    \"oai://gpt-3.5-turbo-0125\"  \n",
    "]\n",
    "\n",
    "models = [\n",
    "    enc_dec_models[0], # bart-large-cnn\n",
    "    dec_models[0], # Mistral-7B-Instruct-v0.3\n",
    "    gpts[0] # gpt-4o-mini\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af4e7-5a02-4e5a-b2f5-a065716ddbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model names \n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74658955-4a41-4be1-b29d-8efa734bed9d",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73f6a1-7be9-43a7-8304-4cb6a408318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following to 0 to use all samples in the dataset\n",
    "max_samples = 10\n",
    "\n",
    "responses = []\n",
    "for model in models:\n",
    "    descr = f\"Testing {model} summarization model on {dataset_name}\"\n",
    "    responses.append(ld.experiments_submit(model, team_name, descr, dataset_id, max_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787190-4581-4c26-beca-0260303a68bd",
   "metadata": {},
   "source": [
    "### Track evaluation jobs\n",
    "\n",
    "Run the following to track your evaluation jobs.\n",
    "\n",
    "- *NOTE*: you won't be able to run other cells while this one is running. However, you can interrupt it whenever you want by clicking on the \"stop\" button above and run it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858bcea-6fcb-4441-b56e-5d89502f10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [ld.get_resource_id(r) for r in responses]\n",
    "\n",
    "wip = ld.show_experiment_statuses(job_ids)\n",
    "while wip == True:\n",
    "    time.sleep(5)\n",
    "    clear_output()\n",
    "    wip=ld.show_experiment_statuses(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8cbf3-e44f-4f1e-b520-3f6fac147fca",
   "metadata": {},
   "source": [
    "## Show evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709b8-59ba-4a9c-81bd-59b2fc612f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the jobs complete, gather evaluation results\n",
    "eval_results = []\n",
    "for job_id in job_ids:\n",
    "    eval_results.append(ld.experiments_result_download(job_id))\n",
    "\n",
    "# convert results into a pandas dataframe\n",
    "eval_table = ld.eval_results_to_table(models, eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf5e90-7fa2-4a7d-98bf-2ff5d736c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbdaec-a1e9-4731-bfdd-b471e6a0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
