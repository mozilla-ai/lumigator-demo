{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3373f2c7-da8e-49eb-8300-25ae4a00fdaa",
   "metadata": {},
   "source": [
    "# Welcome to Lumigator Foxfooding from [Mozilla.ai](https://www.mozilla.ai/)! üêä ü¶ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe362e97",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "+ Working with Jupyter Notebooks\n",
    "+ Lumigator Platform Overview  üêä\n",
    "+ Understanding Machine Learning Workflows \n",
    "+ Thunderbird Dataset Walkthrough\n",
    "+ Explanation of and Examination of Thunderbird Ground Truth\n",
    "+ Model Selection ( 1 encoder/decoder), (2 decoder), eval against GPT4\n",
    "+ Run experiment and show results\n",
    "+ Evaluate results and discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41dab1",
   "metadata": {},
   "source": [
    "## Jupyter Walkthrough\n",
    "\n",
    "[Jupyter Notebooks](https://jupyter-notebook.readthedocs.io/en/stable/) are an executable code/text environment for (mostly) Python code. Our Jupyter environment is on JupyterHub, which you've launched at startup. To work with Jupyter, click \"run cell\" to run the code and see results below the cell you're currently running. Cells are executed sequentially "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecaa846",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Some terms you'll hear us using throughout the session: \n",
    "\n",
    "+ **LLM** - Large language model, a text-based model that performs next-word predictions \n",
    "+ **Tokens** - Words broken up into pieces to be used in an LLM \n",
    "+ **Inference** - The process of getting a prediction from a large language model \n",
    "+ **Embeddings** - Numerical representations of text generated by modeling\n",
    "+ **Encoder-decoder models** - a neural network architecture comprised of two neural networks, an encoder that takes the input vectors from our data and creates an embedding of a fixed length, and a decoder, also a neural network, which takes the embeddings encoded as input and generates a static set of outputs such as translated text or a text summary\n",
    "+ **Decoder-only models** - Receive a prompt of text directly and predict the next word\n",
    "+ **Task** - Machine learning tasks to fit a specific model type, including translation, summarization, completion, etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334451d",
   "metadata": {},
   "source": [
    "## Lumigator Platform Overview\n",
    "\n",
    "TODO: Add URL\n",
    "\n",
    "You'll be working in the Jupyter notebook and the platform itself is accessible via URL in the slide deck. The app itself consists of an API, which you can access and test out methods with in the [OpenAPI spec](https://swagger.io/specification/), at the platform URL, under docs. \n",
    "\n",
    "<img src=\"openapi.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfaf888",
   "metadata": {},
   "source": [
    "# Machine Learning Workflows\n",
    "\n",
    "Generally, when working with machine learning, we are looking to generate a model artifact from data. We have several stages we care about: the data preprocessing, model training, model generation, inference, and evaluation. \n",
    "\n",
    "<img src=\"ml_workflow.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Within the universe of modeling approaches, there are supervised and unsupervised approaches, as well as reinforcement learning. When we think of language modeling, that falls in the realm of neural network approaches. \n",
    "\n",
    "\n",
    "<img src=\"ml_family.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Lumigator focuses on **inference** and **evaluation** for large language models: we want to be able to take our own dataset, perform inference on it, and evaluate the results to see if the model we would like to use produces good results for our use-cases. Use-cases include cases that are specific to our business. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d2fd4",
   "metadata": {},
   "source": [
    "## Machine learning is alchemy!\n",
    "\n",
    "When we think of traditional application workflows, we think of an example such as adding a button. We can clearly test that we've added a blue button to our application, and that it works correctly. Machine learning is not like this! It involves a lot of experimentation, tweaking of hyperparameters and prompts and trying different models. Expect for the process to be imperfect, with many iterative loops. Luckily, Lumigator helps take away the uncertainty of at least model selection :)\n",
    "\n",
    "> There‚Äôs a self-congratulatory feeling in the air. We say things like ‚Äúmachine learning is the new electricity‚Äù. I‚Äôd like to offer an alternative metaphor: machine learning has become alchemy. - [Ben Recht and Ali Rahmi](https://archives.argmin.net/2017/12/05/kitchen-sinks/)\n",
    "\n",
    "\n",
    "Ultimately, the final conclusion of whether a model is good is if humans think it's good. \n",
    "\n",
    "With that in mind, let's dive into setting up experiments with Lumigator to test our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8237ee62-33d1-480d-864f-03f70537c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import lumigator_demo as ld\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wrap columns for inspection\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# stylesheet for visibility\n",
    "plt.style.use(\"fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c813-36f8-4439-880d-441499e80493",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "We're grouping experiments by team name to organize the data, pick a team name below and run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67092b57-e4ff-4fd7-a46b-f6ab9c56c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestion: \"lumigator_enthusiasts\", \"your team name etc\"\n",
    "team_name = TEAM_NAME_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e0905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "What are LLMS? what are encoder-decoder, decoder models, working with text and tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beee984",
   "metadata": {},
   "source": [
    "## Ground Truth for Models\n",
    "[Generating GT](https://thunderbird.topicbox.com/groups/addons/T18e84db141355abd-M4cca8e3f9e4fee9ae14b9dbb/self-hosted-version-of-extension-is-incorrectly-appearing-in-atn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1493b-ecbd-4b2c-a13a-58365cc15023",
   "metadata": {},
   "source": [
    "## Generating Data for Ground Truth Evaluation\n",
    "\n",
    "In order to generate a ground truth summary for our data, we first need an input dataset. In this case we use threads from the [Thunderbird public mailing list.](https://thunderbird.topicbox.com/latest).  In order to generate the ground truth and then later evaluate the model, we need at least 100 samples to start with, where a sample is a single email or single email conversation.\n",
    "\n",
    "Our selection criteria: \n",
    "\n",
    "+ Collect 100 samples of email thread conversations, as recent as possible and fairly complete so they can be evaluated\n",
    "+ Clean them of email formatting such as `>`\n",
    "+ One consideration here will be that BART, the baseline model we're using, accepts 1024 token context window as input, i.e.  we have to have input email threads that are ~ approximately 1000 words, so keeping on the conservative side\n",
    "\n",
    "Once we've collected them, we'd like to take a look at the data before we generate summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36323a9e-aec4-402f-81c0-164f4f890723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T17:59:45.160392Z",
     "start_time": "2024-07-17T17:59:45.157516Z"
    }
   },
   "outputs": [],
   "source": [
    "# show information about the Thunderbird dataset\n",
    "dataset_id = \"db7ff8c2-a255-4d75-915d-77ba73affc53\"\n",
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab1a6c-30bc-421d-908d-20268cdefb98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:10.371832Z",
     "start_time": "2024-07-17T18:00:10.354800Z"
    }
   },
   "outputs": [],
   "source": [
    "# download the dataset into a pandas dataframe\n",
    "df = ld.dataset_download(dataset_id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb735b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_text(text:str):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n",
    "    text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df[\"examples\"].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ddccc-33ab-4204-967a-d266b8ff051e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:25.102809Z",
     "start_time": "2024-07-17T18:00:25.098826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "# we define the data with examples\n",
    "df['examples'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a4fa9-ce82-43ec-a2b1-0fc984b23cea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:28.965534Z",
     "start_time": "2024-07-17T18:00:28.962726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a function to do some simple character counts for model input\n",
    "df['char_count'] = df['examples'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1336138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9694e3-d4d3-46b6-9023-4744f1ab73e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:31.627488Z",
     "start_time": "2024-07-17T18:00:31.620802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28c016-5aae-47f2-a18e-e470cae8ef4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T18:00:33.594470Z",
     "start_time": "2024-07-17T18:00:33.477793Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "what is ground truth? how do we vibe-compare it between models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe26e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with Mistral \n",
    "\n",
    "responses = []\n",
    "\n",
    "for sample in df['examples'][0:100]:\n",
    "    response = ld.get_mistral_ground_truth(sample)\n",
    "    print(f\"Response from Mistral\", response[:10])\n",
    "    responses.append((sample, response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_df = pd.DataFrame(responses, columns=['Original', 'Response'])\n",
    "\n",
    "mistral_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f9648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "ways to run LLMs: API access, running on cluster, and running locally \n",
    "what is Ray? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "Lumigator and LM-Buddy, how they work together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all deployments \n",
    "ld.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f48f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Ground Truth Generation with BART\n",
    "\n",
    "# PUT DEPLOYMENT ID HERE \n",
    "deployment_id = \"3b3a7f4d-0a4f-4ff1-b760-2c126279bc00\"\n",
    "\n",
    "for string in df['examples'][0:1]:\n",
    "    response = ld.get_bart_ground_truth(deployment_id,\"hello\")\n",
    "    print(string, response)\n",
    "    responses.append((string, response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e914253-e217-42f8-85a2-59f32daf6779",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66191-c874-4303-bd26-ef93388a124f",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "The following dataset is already in the format that we need as input: \n",
    "- one field called `examples` containing the text to summarize\n",
    "- one field called `ground_truth` containing the summaries to the models' outputs against\n",
    "\n",
    "Note that you can load many different types of file formats in a similar way (see https://huggingface.co/docs/datasets/loading#local-and-remote-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb40cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:\n",
    "\n",
    "huggingface datasets versus csv\n",
    "and lm-buddy prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0682d-0dd6-49b1-a4dc-8581760b1a59",
   "metadata": {},
   "source": [
    "## Dataset Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fbcb60-5940-41e5-873f-b346f8bad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"thunderbird.csv\"\n",
    "dataset_id = \"f5d54efa-247d-4910-9393-f6003da9fb68\" # thunderbird pre-saved dataset HuggingFace\n",
    "\n",
    "r = ld.dataset_info(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876b7f8-e645-4791-aa27-351e3296e2de",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "What you see below are different lists of models we have already tested for the summarization task.\n",
    "The `models` variable at the end provides you with a selection, but you can choose any combination of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcdf76-c9ad-4459-b0c3-49098286ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_models = [\n",
    "    'hf://facebook/bart-large-cnn',\n",
    "    'hf://mikeadimech/longformer-qmsum-meeting-summarization', \n",
    "    'hf://mrm8488/t5-base-finetuned-summarize-news',\n",
    "    'hf://Falconsai/text_summarization',\n",
    "]\n",
    "\n",
    "dec_models = [\n",
    "    'mistral://open-mistral-7b',\n",
    "]\n",
    "\n",
    "gpts = [\n",
    "    \"oai://gpt-4o-mini\",\n",
    "    \"oai://gpt-4-turbo\",\n",
    "    \"oai://gpt-3.5-turbo-0125\"  \n",
    "]\n",
    "\n",
    "# TODO: add llamafile\n",
    "\n",
    "models = [\n",
    "    enc_dec_models[0], # bart-large-cnn\n",
    "    dec_models[0], # Mistral-7B-Instruct-v0.3\n",
    "    gpts[0] # gpt-4o-mini\n",
    "]\n",
    "\n",
    "# show selected models\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "Introduce metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74658955-4a41-4be1-b29d-8efa734bed9d",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73f6a1-7be9-43a7-8304-4cb6a408318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following to 0 to use all samples in the dataset\n",
    "max_samples = 10\n",
    "\n",
    "responses = []\n",
    "for model in models:\n",
    "    descr = f\"Testing {model} summarization model on {dataset_name}\"\n",
    "    responses.append(ld.experiments_submit(model, team_name, descr, dataset_id, max_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc890c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "Discuss Ray dashboard/show dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787190-4581-4c26-beca-0260303a68bd",
   "metadata": {},
   "source": [
    "### Track evaluation jobs\n",
    "\n",
    "Run the following to track your evaluation jobs.\n",
    "\n",
    "- *NOTE*: you won't be able to run other cells while this one is running. However, you can interrupt it whenever you want by clicking on the \"stop\" button above and run it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858bcea-6fcb-4441-b56e-5d89502f10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [ld.get_resource_id(r) for r in responses]\n",
    "\n",
    "wip = ld.show_experiment_statuses(job_ids)\n",
    "while wip == True:\n",
    "    time.sleep(5)\n",
    "    clear_output()\n",
    "    wip=ld.show_experiment_statuses(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8cbf3-e44f-4f1e-b520-3f6fac147fca",
   "metadata": {},
   "source": [
    "## Show evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709b8-59ba-4a9c-81bd-59b2fc612f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the jobs complete, gather evaluation results\n",
    "eval_results = []\n",
    "for job_id in job_ids:\n",
    "    eval_results.append(ld.experiments_result_download(job_id))\n",
    "\n",
    "# convert results into a pandas dataframe\n",
    "eval_table = ld.eval_results_to_table(models, eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf5e90-7fa2-4a7d-98bf-2ff5d736c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbdaec-a1e9-4731-bfdd-b471e6a0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b450ce",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea7929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "add eval discussion "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
